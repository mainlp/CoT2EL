# Rank-based Human Label Variation Evaluation

This project provides a comprehensive framework for evaluating the quality of AI-generated explanations for MCQA tasks using an LLM-as-a-Judge approach. It takes the structured, filtered explanations from the `Pipeline` project as input, uses a powerful model like Qwen to assess them, and calculates a suite of metrics against a human-annotated gold standard.


## Project Structure

```bash 
hlv_evaluation_pipeline/ 
├── main_evaluator.py        # Main entry point to run the evaluation pipeline 
├── evaluator.py             # Core logic for running the LLM-as-a-Judge model 
├── data_processor.py        # Pre-processes LLM outputs and gold standard data 
├── metrics_calculator.py    # Contains all metric calculation functions
├── prompt_factory_eval.py   # Generates the various prompts for the judge LLM
├── configs_eval/
│ ├── qwen_eval_cqa.yaml   # Example configuration for evaluating CQA with Qwen 
└── README.md 
``` 

## Setup

1.  **Install dependencies:**
    ```bash
    pip install transformers torch pandas numpy scipy scikit-learn dcor
    ```
    *Note: `dcor` might require a C++ compiler during installation.*

2.  **Configure your evaluation task:**
    -   Navigate to the `configs_eval/` directory.
    -   Create or edit a YAML file (e.g., `qwen_eval_cqa.yaml`).
    -   **Crucially, you must fill in all file paths**, including the `input_explanation_file` (the output from the previous project), the `gold_standard_file`, and the desired `output_dir`.
    -   Set your `model_name` and `cache_dir` for the evaluation model.

## How to Run

The pipeline is executed via `main_evaluator.py`. You need to specify the configuration file and the evaluation mode.

### Step 1: Run the LLM-as-a-Judge Evaluation

First, generate the model's evaluations. You can run it in two modes:

1.  **Baseline Mode**: The LLM evaluates the original questions without any explanations.
    ```bash
    python main_evaluator.py --config configs_eval/qwen_eval_cqa.yaml --mode baseline
    ```

2.  **With Explanations Mode**: The LLM evaluates the questions *with* the explanations generated by the previous project.
    ```bash
    python main_evaluator.py --config configs_eval/qwen_eval_cqa.yaml --mode with_explanations
    ```
This step will create a raw output file (e.g., `cqa_with_explanations_raw_output.jsonl`) in your specified output directory.

### Step 2: Process the Outputs and Calculate Metrics

After generating the raw evaluations, run the script in `calculate` mode. This will process both the raw LLM output and the gold standard data, then calculate all metrics and save them to an Excel file.

```bash
python main_evaluator.py --config configs_eval/qwen_eval_cqa.yaml --mode calculate
```

This will generate the final metrics report (e.g., `cqa_metrics_report.xlsx`).
